{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 9:Implementing LeNet-5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMesFPW6HZPmC2dzh7NeTD+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sautrikc/Seasons-of-Code/blob/main/Week_9_Implementing_LeNet_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Based Stock Market Prediction"
      ],
      "metadata": {
        "id": "BuZbzd58Madm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following section, a modified version of LeNet-5 has been implemented to improve the model performance."
      ],
      "metadata": {
        "id": "b9vU-bwNMyLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2D-CNN-pred"
      ],
      "metadata": {
        "id": "9yNRUxV4k---"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Importing important libraries"
      ],
      "metadata": {
        "id": "TPHPxKlnY4F2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC4WJSaqQpvP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error"
      ],
      "metadata": {
        "id": "9pqqy6bkQqhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Preprocessing"
      ],
      "metadata": {
        "id": "vWDr-ChWZA74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is split into training and testing set. Further, the training set is divided into training and validation."
      ],
      "metadata": {
        "id": "39xOXkSVcBB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DATADIR = \"./Dataset\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75"
      ],
      "metadata": {
        "id": "FVov7ydcRuJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is organised into dictionaries with the keys of the dictionary set to the name of the stock market index. Further the data is scaled using 'Standard Scaler'."
      ],
      "metadata": {
        "id": "dveDqOlMY3DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {}\n",
        "for filename in os.listdir():\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    #filepath = os.path.join(DATADIR, filename)\n",
        "    X = pd.read_csv(filename, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X"
      ],
      "metadata": {
        "id": "DXvbymH0Y1ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmo-sWhsc1A2",
        "outputId": "48a56d53-25de-4654-db28-c5f323248401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['NASDAQ', 'NYA', 'S&P', 'RUT', 'DJI'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A data generator is formed which can produce batches of data when called by our model."
      ],
      "metadata": {
        "id": "nEZ0-absliUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    batch = []\n",
        "    while True:\n",
        "        # Pick one dataframe from the pool\n",
        "        key = random.choice(list(data.keys()))\n",
        "        df = data[key]\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
        "        if kind == 'train':\n",
        "            index = index[:split]   # range for the training set\n",
        "        elif kind == 'valid':\n",
        "            index = index[split:]   # range for the validation set\n",
        "        # Pick one position, then clip a sequence length\n",
        "        while True:\n",
        "            t = random.choice(index)      # pick one time step\n",
        "            n = (df.index == t).argmax()  # find its position in the dataframe\n",
        "            if n-seq_len+1 < 0:\n",
        "                continue # can't get enough data for one sequence length\n",
        "            frame = df.iloc[n-seq_len+1:n+1]\n",
        "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
        "            break\n",
        "        # if we get enough for a batch, dispatch\n",
        "        if len(batch) == batch_size:\n",
        "            X, y = zip(*batch)\n",
        "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
        "            yield X, y\n",
        "            batch = []"
      ],
      "metadata": {
        "id": "bKhyYJIMli35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "talq0Oytacz0",
        "outputId": "1defee61-aa63-45b5-e0f1-3731dc6518c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'Processed_NASDAQ.csv',\n",
              " 'Processed_NYSE.csv',\n",
              " 'Processed_S&P.csv',\n",
              " 'Processed_RUSSELL.csv',\n",
              " 'Processed_DJI.csv',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Model Formulation"
      ],
      "metadata": {
        "id": "h9pWg171koFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In my implementation of the LeNet-5 for stock market prediction, I first used a convolution layer (1x82) with 8 filters for extracting most important features. This was followed by convolution layer with size (3x1) for extracting durational features. I did not use 5x5 as previous literature uses three consecutive days in case of financial market studies. Instead of average pooling layer I used max pooling. This was followed by another set of convolution and max pooling layers. The droprate was set to 0.1 and the layers flattened to be input into the dense network with 120, 84 and 1 number of neurons. Tanh was used in all the activation layers except for the last where sigmoid was used."
      ],
      "metadata": {
        "id": "zFMBE2zAdZ3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cnnpred_2d(seq_len=60, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
        "    \"2D-CNNpred model according to the paper\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(seq_len, n_features, 1)),\n",
        "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
        "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"tanh\"),\n",
        "        MaxPool2D(pool_size=(2,1)),\n",
        "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"tanh\"),\n",
        "        MaxPool2D(pool_size=(2,1)),\n",
        "        Flatten(),\n",
        "        Dropout(droprate),\n",
        "        Dense(120, activation=\"tanh\"),\n",
        "        Dense(84, activation=\"tanh\"),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "xONMHr7kaxXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The metrics to evaluate our model are F1 score and accuracy. The mean absolute error is used as the loss function."
      ],
      "metadata": {
        "id": "UpiTfC5GfR00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2"
      ],
      "metadata": {
        "id": "cWbke25CGwXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Model Training"
      ],
      "metadata": {
        "id": "rYyO2AhckuoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 60\n",
        "batch_size = 128\n",
        "n_epochs = 20\n",
        "n_features = 82\n",
        " \n",
        "# Produce CNNpred as a binary classification problem\n",
        "model = cnnpred_2d(seq_len, n_features)\n",
        "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
        "model.summary()  # print model structure to console\n",
        " \n",
        "# Set up callbacks and fit the model\n",
        "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
        "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
        "callbacks = [\n",
        "    ModelCheckpoint(checkpoint_path,\n",
        "                    monitor='val_f1macro', mode=\"max\",\n",
        "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
        "]\n",
        "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
        "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
        "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16p2PTH4ISQk",
        "outputId": "13cfc450-cc55-4abe-e5ca-c991e6c67cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 60, 1, 8)          664       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 58, 1, 8)          200       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 29, 1, 8)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 27, 1, 8)          200       \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 13, 1, 8)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 104)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 104)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 120)               12600     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 85        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,913\n",
            "Trainable params: 23,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "400/400 [==============================] - 66s 162ms/step - loss: 0.3340 - acc: 0.6701 - f1macro: 0.5928 - val_loss: 0.5054 - val_acc: 0.4938 - val_f1macro: 0.4640\n",
            "Epoch 2/20\n",
            "400/400 [==============================] - 66s 166ms/step - loss: 0.2162 - acc: 0.7858 - f1macro: 0.7733 - val_loss: 0.5188 - val_acc: 0.4805 - val_f1macro: 0.4249\n",
            "Epoch 3/20\n",
            "400/400 [==============================] - 64s 161ms/step - loss: 0.1948 - acc: 0.8060 - f1macro: 0.7954 - val_loss: 0.4882 - val_acc: 0.5125 - val_f1macro: 0.4705\n",
            "Epoch 4/20\n",
            "400/400 [==============================] - 64s 159ms/step - loss: 0.1796 - acc: 0.8207 - f1macro: 0.8116 - val_loss: 0.5498 - val_acc: 0.4477 - val_f1macro: 0.4273\n",
            "Epoch 5/20\n",
            "400/400 [==============================] - 65s 163ms/step - loss: 0.1703 - acc: 0.8304 - f1macro: 0.8225 - val_loss: 0.5015 - val_acc: 0.4961 - val_f1macro: 0.4615\n",
            "Epoch 6/20\n",
            "400/400 [==============================] - 64s 160ms/step - loss: 0.1658 - acc: 0.8345 - f1macro: 0.8275 - val_loss: 0.4932 - val_acc: 0.5086 - val_f1macro: 0.4952\n",
            "Epoch 7/20\n",
            "400/400 [==============================] - 65s 162ms/step - loss: 0.1648 - acc: 0.8354 - f1macro: 0.8287 - val_loss: 0.5348 - val_acc: 0.4672 - val_f1macro: 0.4169\n",
            "Epoch 8/20\n",
            "400/400 [==============================] - 64s 160ms/step - loss: 0.1608 - acc: 0.8394 - f1macro: 0.8322 - val_loss: 0.5061 - val_acc: 0.4922 - val_f1macro: 0.4483\n",
            "Epoch 9/20\n",
            "400/400 [==============================] - 64s 161ms/step - loss: 0.1596 - acc: 0.8403 - f1macro: 0.8336 - val_loss: 0.5457 - val_acc: 0.4578 - val_f1macro: 0.4335\n",
            "Epoch 10/20\n",
            "400/400 [==============================] - 65s 163ms/step - loss: 0.1579 - acc: 0.8422 - f1macro: 0.8354 - val_loss: 0.4792 - val_acc: 0.5188 - val_f1macro: 0.4849\n",
            "Epoch 11/20\n",
            "400/400 [==============================] - 68s 169ms/step - loss: 0.1536 - acc: 0.8463 - f1macro: 0.8401 - val_loss: 0.4785 - val_acc: 0.5227 - val_f1macro: 0.4821\n",
            "Epoch 12/20\n",
            "400/400 [==============================] - 65s 163ms/step - loss: 0.1471 - acc: 0.8528 - f1macro: 0.8469 - val_loss: 0.5428 - val_acc: 0.4578 - val_f1macro: 0.4229\n",
            "Epoch 13/20\n",
            "400/400 [==============================] - 67s 167ms/step - loss: 0.1533 - acc: 0.8468 - f1macro: 0.8403 - val_loss: 0.5207 - val_acc: 0.4797 - val_f1macro: 0.4530\n",
            "Epoch 14/20\n",
            "400/400 [==============================] - 68s 170ms/step - loss: 0.1454 - acc: 0.8548 - f1macro: 0.8491 - val_loss: 0.4831 - val_acc: 0.5188 - val_f1macro: 0.4986\n",
            "Epoch 15/20\n",
            "400/400 [==============================] - 69s 172ms/step - loss: 0.1429 - acc: 0.8572 - f1macro: 0.8513 - val_loss: 0.5135 - val_acc: 0.4859 - val_f1macro: 0.4437\n",
            "Epoch 16/20\n",
            "400/400 [==============================] - 67s 167ms/step - loss: 0.1432 - acc: 0.8570 - f1macro: 0.8516 - val_loss: 0.5248 - val_acc: 0.4742 - val_f1macro: 0.4518\n",
            "Epoch 17/20\n",
            "400/400 [==============================] - 65s 163ms/step - loss: 0.1435 - acc: 0.8564 - f1macro: 0.8515 - val_loss: 0.5288 - val_acc: 0.4695 - val_f1macro: 0.4356\n",
            "Epoch 18/20\n",
            "400/400 [==============================] - 65s 163ms/step - loss: 0.1396 - acc: 0.8604 - f1macro: 0.8552 - val_loss: 0.5161 - val_acc: 0.4828 - val_f1macro: 0.4651\n",
            "Epoch 19/20\n",
            "400/400 [==============================] - 65s 163ms/step - loss: 0.1399 - acc: 0.8601 - f1macro: 0.8550 - val_loss: 0.5346 - val_acc: 0.4641 - val_f1macro: 0.4288\n",
            "Epoch 20/20\n",
            "400/400 [==============================] - 64s 159ms/step - loss: 0.1346 - acc: 0.8654 - f1macro: 0.8603 - val_loss: 0.5489 - val_acc: 0.4492 - val_f1macro: 0.4401\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcc73bcb590>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A data generator for the test set is also prepared. The mean absolute error, accuracy and F1 score are measured and reported below."
      ],
      "metadata": {
        "id": "UYGBtI_igDLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Model Testing"
      ],
      "metadata": {
        "id": "tjtf9Tcukzo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testgen(data, seq_len, targetcol):\n",
        "    \"Return array of all test samples\"\n",
        "    batch = []\n",
        "    for key, df in data.items():\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        # find the start of test sample\n",
        "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
        "        n = (df.index == t).argmax()\n",
        "        for i in range(n+1, len(df)+1):\n",
        "            frame = df.iloc[i-seq_len:i]\n",
        "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
        "    X, y = zip(*batch)\n",
        "    return np.expand_dims(np.array(X),3), np.array(y)\n",
        "\n",
        "# Prepare test data\n",
        "test_data, test_target = testgen(data, seq_len, \"Target\")\n",
        "\n",
        "# Test the model\n",
        "test_out = model.predict(test_data)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
        "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
        "print(\"F1:\", f1_score(test_pred, test_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhNM5L20KMKe",
        "outputId": "1af0cb63-2d91-4b2f-8da6-9a83d7d53728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.5707317073170731\n",
            "MAE: 0.4292682926829268\n",
            "F1: 0.6147110332749562\n"
          ]
        }
      ]
    }
  ]
}